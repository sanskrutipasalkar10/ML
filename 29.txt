1. 
from sklearn.decomposition import PCA
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
import matplotlib.pyplot as plt

# Load the Iris dataset
iris = load_iris()
X = iris.data  # Features: sepal length, sepal width, petal length, petal width
y = iris.target  # Target: flower species (0, 1, 2)

# Apply PCA to reduce the data to 2D
pca = PCA(n_components=2)
X_pca = pca.fit_transform(X)

# Visualize the reduced data
plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis')
plt.title('2D PCA of Iris Dataset')
plt.xlabel('Principal Component 1')
plt.ylabel('Principal Component 2')
plt.colorbar(label='Flower Species')
plt.show()

# Split the dataset into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.3, random_state=42)

# Train a classifier (SVM) on the reduced 2D data
svm = SVC(kernel='linear')
svm.fit(X_train, y_train)

# Predict the flower species for the test data
y_pred = svm.predict(X_test)

# Evaluate the model
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy of the SVM classifier with PCA-reduced data: {accuracy * 100:.2f}%")

# Predict the flower species for new data point (given measurements)
new_data = [[5.1, 3.5, 1.4, 0.2]]  # Example input: [sepal length, sepal width, petal length, petal width]
new_data_pca = pca.transform(new_data)  # Apply PCA transformation
predicted_species = svm.predict(new_data_pca)
print(f"Predicted species for the new flower: {iris.target_names[predicted_species[0]]}")


2

import pandas as pd
import numpy as np
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score
import matplotlib.pyplot as plt

# Example Employee Dataset (You can replace it with your own dataset)
data = {
    'Age': [25, 35, 45, 50, 23, 38, 60, 29, 40, 55],
    'Years of Experience': [1, 10, 20, 25, 2, 8, 35, 5, 15, 30],
    'Salary': [30000, 60000, 100000, 120000, 28000, 65000, 150000, 35000, 75000, 110000]
}

# Create a DataFrame
df = pd.DataFrame(data)

# Preprocess: Standardize the data
scaler = StandardScaler()
X_scaled = scaler.fit_transform(df)

# Elbow Method to find the optimal number of clusters (K)
inertia = []
silhouette_scores = []
for k in range(2, 11):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(X_scaled)
    inertia.append(kmeans.inertia_)
    silhouette_scores.append(silhouette_score(X_scaled, kmeans.labels_))

# Plot the Elbow Method graph
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), inertia, marker='o', linestyle='-', color='b')
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.show()

# Plot the Silhouette Scores
plt.figure(figsize=(10, 6))
plt.plot(range(2, 11), silhouette_scores, marker='o', linestyle='-', color='r')
plt.title('Silhouette Scores for Optimal K')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Silhouette Score')
plt.show()

# Use the optimal K (let's assume it is K=3 from elbow and silhouette scores)
optimal_k = 3
kmeans_optimal = KMeans(n_clusters=optimal_k, random_state=42)
kmeans_optimal.fit(X_scaled)

# Assign clusters to each employee
df['Cluster'] = kmeans_optimal.labels_

# Display the clustered data
print("Clustered Employees Data:")
print(df)

# Visualizing the clusters (Optional)
plt.scatter(df['Age'], df['Salary'], c=df['Cluster'], cmap='viridis')
plt.title('Employee Clusters based on Age and Salary')
plt.xlabel('Age')
plt.ylabel('Salary')
plt.colorbar(label='Cluster')
plt.show()
