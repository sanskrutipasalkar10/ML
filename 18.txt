1.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.cluster import KMeans
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import silhouette_score

# Load the Diabetes dataset (Pima Indians Diabetes Database)
url = "https://raw.githubusercontent.com/jbrownlee/Datasets/master/pima-indians-diabetes.data.csv"
column_names = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI', 'DiabetesPedigreeFunction', 'Age', 'Outcome']
data = pd.read_csv(url, names=column_names)

# Prepare the feature matrix X and target variable y (Outcome is not needed for clustering)
X = data.drop(columns='Outcome')

# Scale the features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Apply K-Means clustering
kmeans = KMeans(n_clusters=2, random_state=42)  # We assume two clusters (Diabetic, Non-Diabetic)
kmeans.fit(X_scaled)

# Get the cluster centroids and labels
centroids = kmeans.cluster_centers_
labels = kmeans.labels_

# Evaluate the clustering using silhouette score
silhouette = silhouette_score(X_scaled, labels)
print(f"Silhouette Score: {silhouette:.2f}")

# Plot the clusters
plt.figure(figsize=(10, 6))

# Scatter plot with clusters
plt.scatter(X_scaled[:, 0], X_scaled[:, 1], c=labels, cmap='viridis', alpha=0.5)
plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='red', marker='x', label="Centroids")

plt.title("K-Means Clustering on Diabetes Dataset")
plt.xlabel("Feature 1: Pregnancies")
plt.ylabel("Feature 2: Glucose")
plt.legend()
plt.show()


2.

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.linear_model import LinearRegression
from sklearn.preprocessing import PolynomialFeatures
from sklearn.model_selection import train_test_split

# Sample Salary Positions Dataset (Example data)
data = {
    'Level': [1, 2, 3, 4, 5, 6, 7, 8, 9, 10],
    'Salary': [45000, 50000, 60000, 65000, 70000, 80000, 85000, 90000, 95000, 100000]
}
df = pd.DataFrame(data)

# Prepare the features and target variable
X = df[['Level']].values  # Level is the feature
y = df['Salary'].values   # Salary is the target variable

# Split the data into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Apply Polynomial Feature Transformation (Degree=3)
poly = PolynomialFeatures(degree=3)
X_poly_train = poly.fit_transform(X_train)
X_poly_test = poly.transform(X_test)

# Create and fit the Polynomial Regression model
poly_reg_model = LinearRegression()
poly_reg_model.fit(X_poly_train, y_train)

# Predict the salaries on the test data
y_pred = poly_reg_model.predict(X_poly_test)

# Evaluate the performance (Optional: Mean Squared Error or R-squared)
from sklearn.metrics import mean_squared_error, r2_score
mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

# Print evaluation results
print(f"Mean Squared Error: {mse:.2f}")
print(f"R-squared: {r2:.2f}")

# Predict salaries for level 11 and level 12
level_11 = poly.transform([[11]])
level_12 = poly.transform([[12]])
salary_11 = poly_reg_model.predict(level_11)
salary_12 = poly_reg_model.predict(level_12)

print(f"Predicted Salary for Level 11: ${salary_11[0]:,.2f}")
print(f"Predicted Salary for Level 12: ${salary_12[0]:,.2f}")

# Visualize the results
plt.scatter(X, y, color='red')  # Scatter plot of original data
plt.plot(X, poly_reg_model.predict(poly.fit_transform(X)), color='blue')  # Polynomial regression line
plt.title("Polynomial Linear Regression for Salary Prediction")
plt.xlabel("Position Level")
plt.ylabel("Salary")
plt.show()
