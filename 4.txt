1.

# Importing necessary libraries
import pandas as pd
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt

# Load the dataset (replace 'mall_customers.csv' with your actual dataset file)
df = pd.read_csv('mall_customers.csv')

# Display the first few rows of the dataset
print(df.head())

# Assume the dataset contains features like 'Annual Income' and 'Spending Score' for clustering
X = df[['Annual Income (k$)', 'Spending Score (1-100)']]  # Replace with actual feature columns

# Use the elbow method to find the optimal number of clusters
wcss = []
for i in range(1, 11):  # Trying cluster values from 1 to 10
    kmeans = KMeans(n_clusters=i, init='k-means++', max_iter=300, n_init=10, random_state=42)
    kmeans.fit(X)
    wcss.append(kmeans.inertia_)

# Plot the elbow graph
plt.plot(range(1, 11), wcss)
plt.title('Elbow Method for Optimal K')
plt.xlabel('Number of clusters')
plt.ylabel('WCSS (Within-cluster sum of squares)')
plt.show()

# Apply K-Means algorithm with the optimal number of clusters (let's assume it's 5 based on the elbow method)
kmeans = KMeans(n_clusters=5, init='k-means++', max_iter=300, n_init=10, random_state=42)
y_kmeans = kmeans.fit_predict(X)

# Add cluster labels to the dataset
df['Cluster'] = y_kmeans

# Visualize the clusters
plt.scatter(X['Annual Income (k$)'], X['Spending Score (1-100)'], c=y_kmeans, cmap='viridis')
plt.title('Clusters of Mall Customers')
plt.xlabel('Annual Income (k$)')
plt.ylabel('Spending Score (1-100)')
plt.show()

# Display the resulting clusters with cluster centers
print("Cluster Centers: \n", kmeans.cluster_centers_)


2.

import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score

df = pd.read_csv('C:\\Users\\sanskruti\\OneDrive\\Desktop\\symsc prac\\SEM 3 ML\\HousePriceData.csv')

# Display the number of null values in each column
print("Null values in each column before removing:")
print(df.isnull().sum())

# Remove rows with null values
df = df.dropna()

# Display the number of null values after removing
print("Null values in each column after removing:")
print(df.isnull().sum())

X = df[['FloorArea', 'BedRooms', 'Price']] # Features
y = df['Price'] # Target variable

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2,random_state=42)

model = LinearRegression()
model.fit(X_train, y_train)

y_pred = model.predict(X_test)

mse = mean_squared_error(y_test, y_pred)
r2 = r2_score(y_test, y_pred)

print("Mean Squared Error:", mse)
print("R2 Score:", r2)

predicted_prices = model.predict(X_test)

print("Predicted Prices:", predicted_prices)
print("Actual Prices:", y_test.values)
