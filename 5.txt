1

# Importing necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import matplotlib.pyplot as plt

# Load the dataset (replace 'fuel_consumption.csv' with your actual file)
df = pd.read_csv('fuel_consumption.csv')

# Display the first few rows of the dataset
print(df.head())

# Display the number of null values in each column and remove them if present
print("Null values in each column before removing:")
print(df.isnull().sum())
df = df.dropna()

# Assume the dataset has columns like 'engine_size', 'curb_weight', 'horsepower', and 'fuel_consumption'
# Adjust the features and target column names based on the actual dataset
X = df[['engine_size', 'curb_weight', 'horsepower']]  # Replace with actual feature columns
y = df['fuel_consumption']  # Replace 'fuel_consumption' with the actual target column

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a multiple linear regression model
model = LinearRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Print the model's coefficients
print("Model Coefficients:")
print("Intercept:", model.intercept_)
print("Coefficients:", model.coef_)

# Evaluate the model
print("Mean Squared Error:", mean_squared_error(y_test, y_pred))
print("R^2 Score:", r2_score(y_test, y_pred))

# Visualize the results (for the first feature)
plt.scatter(X_test['engine_size'], y_test, color='blue', label='Actual')
plt.scatter(X_test['engine_size'], y_pred, color='red', label='Predicted')
plt.title('Multiple Linear Regression: Fuel Consumption Prediction')
plt.xlabel('Engine Size')
plt.ylabel('Fuel Consumption')
plt.legend()
plt.show()



2

# Importing necessary libraries
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score, confusion_matrix, classification_report
import matplotlib.pyplot as plt
from sklearn import datasets

# Load the Iris dataset (it can be directly imported from sklearn)
iris = datasets.load_iris()
df = pd.DataFrame(data=iris.data, columns=iris.feature_names)
df['species'] = iris.target

# Display the first few rows of the dataset
print(df.head())

# Features and target
X = df.drop('species', axis=1)
y = df['species']

# Split the dataset into training and testing sets (80% training, 20% testing)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Create a KNN classifier model with k=3 (you can adjust k)
model = KNeighborsClassifier(n_neighbors=3)

# Train the model
model.fit(X_train, y_train)

# Make predictions on the test set
y_pred = model.predict(X_test)

# Evaluate the model
print("Accuracy Score:", accuracy_score(y_test, y_pred))
print("Confusion Matrix:")
print(confusion_matrix(y_test, y_pred))
print("Classification Report:")
print(classification_report(y_test, y_pred))

# Visualize the data (using the first two features for simplicity)
plt.scatter(X['sepal length (cm)'], X['sepal width (cm)'], c=y, cmap='viridis', marker='o')
plt.title('Iris Dataset: Sepal Length vs Sepal Width')
plt.xlabel('Sepal Length (cm)')
plt.ylabel('Sepal Width (cm)')
plt.colorbar(label='Species')
plt.show()
